{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Read csv files\n",
    "\n",
    "folder_path = 'csv_panama_papers'\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    df_name = csv_file.split('.')[2] if len(csv_file.split('.')) > 2 else None\n",
    "    globals()[df_name] = pd.read_csv(file_path)\n",
    "\n",
    "edges = csv.copy()\n",
    "\n",
    "\n",
    "def data_parser(df_name, db_table_columns):\n",
    "    column_mapping = dict(zip(df_name.columns, db_table_columns))\n",
    "    df_name.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "entity[\"name\"] = entity[\"name\"].fillna(value=\"no_name\")\n",
    "\n",
    "db_entity_columns = [\n",
    "    'entity_id', 'name', 'jurisdiction', 'jurisdiction_description', 'country_code', 'country_name', 'incorporation_date', 'inactivation_date', 'struck_off_date', 'closed_date', 'ibc_ruc', 'status', 'company_type', 'service_provider', 'source_id', 'valid_until', 'note'\n",
    "]\n",
    "\n",
    "data_parser(entity, db_entity_columns)\n",
    "\n",
    "# Roles table\n",
    "roles_table = sorted(edges.loc[edges['TYPE'] == 'officer_of']['link'].unique())\n",
    "roles_df = pd.DataFrame(roles_table, columns=[\"role_type\"]).copy()\n",
    "roles_df.insert(0, 'role_id', range(1, 1 + len(roles_df)))\n",
    "roles_df = roles_df[[\"role_id\", \"role_type\"]]\n",
    "#roles_df.to_sql('roles_2307_2325', engine, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "# Officers table\n",
    "\n",
    "# ! For officers table i had to replace 4 nan values in the name column\n",
    "officer[\"name\"] = officer[\"name\"].fillna(value=\"no_name\")\n",
    "\n",
    "db_officer_columns = [\n",
    "    'officer_id', 'name', 'country_code', 'country_name', 'source_id', 'valid_until', 'note'\n",
    "]\n",
    "\n",
    "data_parser(officer, db_officer_columns)\n",
    "\n",
    "\n",
    "# Intermediaries table\n",
    "\n",
    "db_inter_columns = [\n",
    "    'intermediary_id', 'name', 'country_code', 'country_name', 'status', 'source_id', 'valid_until', 'note'\n",
    "]\n",
    "\n",
    "data_parser(intermediary, db_inter_columns)\n",
    "\n",
    "\n",
    "# Preprocessing edges table\n",
    "\n",
    "tmp = edges[edges[\"TYPE\"] == \"officer_of\"].copy()\n",
    "tmp = tmp.merge(roles_df, left_on='link', right_on='role_type').copy()\n",
    "tmp.drop(columns=[\"TYPE\", \"link\", \"role_type\"], inplace=True)\n",
    "tmp = tmp[['START_ID', 'role_id', 'END_ID',\n",
    "           'start_date', 'end_date', 'sourceID', 'valid_until']]\n",
    "\n",
    "# officer_role_entity table\n",
    "\n",
    "officers_roles_entities = tmp[tmp['END_ID'].astype(\n",
    "    str).str.startswith('10')].copy()\n",
    "officers_roles_entities.insert(\n",
    "    0, 'officer_role_entity_id', range(1, 1 + len(officers_roles_entities)))\n",
    "db_ore_columns = [\n",
    "    'officer_role_entity_id', 'officer_id', 'role_id', 'entity_id', 'start_date', 'end_date', 'source_id', 'valid_until'\n",
    "]\n",
    "data_parser(officers_roles_entities, db_ore_columns)\n",
    "# officer_role_officer table\n",
    "\n",
    "officers_roles_officers = tmp[tmp['END_ID'].astype(\n",
    "    str).str.startswith('12')].copy()\n",
    "officers_roles_officers.insert(\n",
    "    0, 'officer_role_officer_id', range(1, 1 + len(officers_roles_officers)))\n",
    "db_oro_columns = [\n",
    "    'officer_role_officer_id', 'officer_id_1', 'role_id', 'officer_id_2', 'start_date', 'end_date', 'source_id', 'valid_until'\n",
    "]\n",
    "data_parser(officers_roles_officers, db_oro_columns)\n",
    "\n",
    "# officer_role_intermediary table\n",
    "\n",
    "officers_roles_intermediaries = tmp[tmp['END_ID'].astype(\n",
    "    str).str.startswith('11')].copy()\n",
    "officers_roles_intermediaries.insert(0, 'officer_role_intermediary_id', range(\n",
    "    1, 1 + len(officers_roles_intermediaries)))\n",
    "db_ori_columns = [\n",
    "    'officer_role_intermediary_id', 'officer_id', 'role_id', 'intermediary_id', 'start_date', 'end_date', 'source_id', 'valid_until'\n",
    "]\n",
    "data_parser(officers_roles_intermediaries, db_ori_columns)\n",
    "\n",
    "\n",
    "# Intermediaries_entities table\n",
    "\n",
    "\n",
    "inter_entity = edges[edges[\"TYPE\"] == \"intermediary_of\"].copy()\n",
    "inter_entity.drop(columns=[\"TYPE\", \"link\"], inplace=True)\n",
    "inter_entity.insert(0, 'intermediary_entity_id',\n",
    "                    range(1, 1 + len(inter_entity)))\n",
    "\n",
    "db_ie_columns = [\n",
    "    'intermediary_entity_id', 'intermediary_id', 'entity_id', 'start_date', 'end_date', 'source_id', 'valid_until'\n",
    "]\n",
    "data_parser(inter_entity, db_ie_columns)\n",
    "\n",
    "\n",
    "# Addresses table\n",
    "\n",
    "db_address_columns = [\n",
    "    'address_id', 'name', 'address', 'country_code', 'country_name', 'source_id', 'valid_until', 'note'\n",
    "]\n",
    "data_parser(address, db_address_columns)\n",
    "\n",
    "\n",
    "# Preprocessing for register_addresses\n",
    "register_addresses = edges[edges[\"TYPE\"] == \"registered_address\"].copy()\n",
    "register_addresses.drop(columns=[\"TYPE\", \"link\"], inplace=True)\n",
    "addresses_entities = register_addresses[register_addresses['START_ID'].astype(\n",
    "    str).str.startswith('10')].copy()\n",
    "addresses_officers = register_addresses[~register_addresses['START_ID'].astype(\n",
    "    str).str.startswith('10')].copy()\n",
    "\n",
    "\n",
    "# Entities_address table\n",
    "\n",
    "addresses_entities.insert(0, 'entity_address_id',\n",
    "                          range(1, 1 + len(addresses_entities)))\n",
    "\n",
    "db_ea_columns = [\n",
    "    'entity_address_id', 'entity_id', 'address_id', 'start_date', 'end_date', 'source_id', 'valid_until'\n",
    "]\n",
    "data_parser(addresses_entities, db_ea_columns)\n",
    "\n",
    "# Officers_address table\n",
    "\n",
    "addresses_officers.insert(0, 'intermediary_entity_id',\n",
    "                          range(1, 1 + len(addresses_officers)))\n",
    "\n",
    "db_oa_columns = [\n",
    "    'officer_address_id', 'officer_id', 'address_id', 'start_date', 'end_date', 'source_id', 'valid_until'\n",
    "]\n",
    "data_parser(addresses_officers, db_oa_columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'user': 'postgres',\n",
    "    'password': 'useruser',\n",
    "    'database': 'test'\n",
    "}\n",
    "\n",
    "# Create a connection to the PostgreSQL database\n",
    "connection = psycopg2.connect(**db_params)\n",
    "\n",
    "# Create a cursor\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Specify the file name for saving SQL statements\n",
    "sql_file = 'tmp.sql'\n",
    "\n",
    "# Define a dictionary mapping DataFrames to table names\n",
    "df_table_mapping = {\n",
    "    'roles_df': 'roles_2307_2325',\n",
    "    'entity': 'entities_2307_2325',\n",
    "    'officer': 'officers_2307_2325',\n",
    "    'address': 'addresses_2307_2325',\n",
    "    'intermediary': 'intermediaries_2307_2325',\n",
    "    'officers_roles_entities': 'officers_roles_entities_2307_2325',\n",
    "    'officers_roles_officers': 'officers_roles_officers_2307_2325',\n",
    "    'officers_roles_intermediaries': 'officers_roles_intermediaries_2307_2325',\n",
    "    'inter_entity': 'intermediaries_entities_2307_2325',\n",
    "    'addresses_entities': 'entities_addresses_2307_2325',\n",
    "    'addresses_officers': 'officers_addresses_2307_2325'\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(sql_file, 'w') as file:\n",
    "    # Iterate over each DataFrame and generate SQL statements\n",
    "    for df_name, table_name in df_table_mapping.items():\n",
    "        # Assuming df_name is the variable name of the DataFrame, fetch the actual DataFrame\n",
    "        df = globals()[df_name]\n",
    "\n",
    "        # Iterate through rows in the DataFrame and generate INSERT statements\n",
    "        for index, row in df.iterrows():\n",
    "            values_str = ', '.join([f\"'{value}'\" if isinstance(value, str) else str(value) for value in row])\n",
    "            insert_statement = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({values_str});\\n\"\n",
    "            file.write(insert_statement)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtsc3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
